# 传统监督学习方法续
## 6. 提升方法 （李航书只写了adaboost，这里参考西瓜书，全补上了。）
提升方法基于一个定理：若一个概念是弱可学习的，那么它必然是强可学习的。 <br>
提升方法的三类基本方式 ：stacking，bagging，boosting。<br>
在提升算法中，基学习器可以是同质的，也可以是异质的。<br>
* 集成学习从一群弱学习器中，学习强学习器，如何保证强学习器的性能呢？西瓜书P173提到，如果弱学习器的错误率相互独立，且错误率小于1/2，那么通过集成学习，可以将错误率降低到0。这个定理是由Hoeffding不等式推导出来的。 <br>
* 但是,由于个体弱学习器都是为了解决同一个问题，所以他们的错误率并不是相互独立的，所以这个定理并不适用于实际情况。需要取舍，达到“好而不同”的效果。 <br> 

基于上述思想，提升方法可以分为两类： <br>
1. Boosting:个体学习器间有强依赖关系，串行生成，每个个体学习器都是在上一个学习器的基础上进行学习，尝试修正上一个学习器的错误。 <br>
2. Bagging:个体学习器间没有强依赖关系，可以并行生成，每个个体学习器都是独立生成的，通过投票等方式进行集成。（Stacking算是他的变种） <br>

### 6.1 Boost
Boosting的思路：从初始训练集T中得到第一个基学习器，然后根据基学习器的表现对训练样本分布进行调整，使得先前基学习器做错的训练样本在后续受到更多关注，然后基于调整后的样本分布得到第二个基学习器，如此重复，直到基学习器数目达到事先指定的值T。再进行加权结合。<br>
#### 6.1.1 AdaBoost：
#### 6.1.2 GBDT
关于GBDT和XGBoost,可以[参考这里。](https://zhuanlan.zhihu.com/p/162001079) <br>
梯度提升决策树，是一种迭代的决策树算法，每次迭代都在训练集上拟合一个回归树，然后根据残差更新模型。 <br>
#### 6.1.3 XGBoost
Extreme Gradient Boosting，是Gradient Boosting的一种高效实现，主要是在损失函数上做了改进，加入了正则项，使得模型更加健壮。 <br>